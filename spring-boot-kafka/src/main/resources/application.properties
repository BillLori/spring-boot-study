#brokers集群
spring.kafka.bootstrap-servers=localhost:9092

#即所有副本都同步到数据时send方法才返回, 以此来完全判断数据是否发送成功, 理论上来讲数据不会丢失.
spring.kafka.producer.acks=all

#发送失败重试次数
spring.kafka.producer.retries=10

#批处理发送条数：当多个记录被发送到同一个分区时，生产者会尝试将记录合并到更少的请求中。这有助于客户端和服务器的性能。
spring.kafka.producer.batch-size=1638

#批处理延迟时间上限：即1ms过后，不管是否达到批处理数，都直接发送一次请求
linger.ms=1

#即32MB的批处理缓冲区
spring.kafka.producer.buffer-memory=33554432

#key-value序列化反序列化
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer

#消费者群组ID，发布-订阅模式，即如果一个生产者，多个消费者都要消费，那么需要定义自己的群组，同一群组内的消费者只有一个能消费到消息
spring.kafka.consumer.group-id=order-beta

#如果为true，消费者的偏移量将在后台定期提交。
spring.kafka.consumer.enable-auto-commit=true

#自动将偏移量置为最早的
spring.kafka.consumer.auto-offset-reset=earliest

#如果设置为自动提交（enable.auto.commit=true），这里设置自动提交周期
spring.kafka.consumer.auto-commit-interval=1000ms

#key-value序列化反序列化
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer

#在使用Kafka的组管理时，用于检测消费者故障的超时
session.timeout.ms=15000

#消费监听器容器并发数
spring.kafka.listener.concurrency=5
